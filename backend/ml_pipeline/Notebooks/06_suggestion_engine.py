#!/usr/bin/env python
# coding: utf-8

# ## Imports & Project Setup
# Sets correct project root
# ‚úî Enables imports from src/
# ‚úî Confirms Phase 5 output exists
# ‚úî Prevents silent downstream errors

# In[2]:


# ========== BASIC IMPORTS ==========
import sys
from pathlib import Path
import yaml
import pandas as pd
import numpy as np

# ========== PROJECT ROOT ==========
PROJECT_ROOT = Path("..").resolve()
sys.path.append(str(PROJECT_ROOT))

print("Project root:", PROJECT_ROOT)
print("Combined signals file exists:",
      (PROJECT_ROOT / "results/combined_signals.csv").exists())


# ## Load Config & Decision Thresholds
# ‚úî Loads decision logic from config
# ‚úî Keeps decisions configurable & explainable
# ‚úî Allows easy tuning for different datasets
# 
# Risk ‚â• REJECT_THRESHOLD ‚Üí ‚ùå Reject
# Risk between REVIEW_THRESHOLD and REJECT_THRESHOLD ‚Üí üîç Review
# Risk < REVIEW_THRESHOLD ‚Üí ‚úÖ Keep

# In[4]:


# ========== LOAD CONFIG ==========
with open(PROJECT_ROOT / "config/default.yaml", "r") as f:
    config = yaml.safe_load(f)

# ========== DECISION THRESHOLDS ==========
REJECT_THRESHOLD = config["decision"]["reject_threshold"]

# You can optionally define a review threshold
REVIEW_THRESHOLD = REJECT_THRESHOLD / 2

print("Reject threshold:", REJECT_THRESHOLD)
print("Review threshold:", REVIEW_THRESHOLD)


# ## Load Combined Signals
# ‚úî Loads risk scores for all samples
# ‚úî Prepares data for decision making
# ‚úî Keeps pipeline modular

# In[5]:


# ========== LOAD COMBINED SIGNALS ==========
combined_path = PROJECT_ROOT / "results/combined_signals.csv"
combined_df = pd.read_csv(combined_path)

print("Total samples:", len(combined_df))
combined_df.head()


# ## Generate Suggestions (CORE DECISION LOGIC)
# #### This cell converts the continuous risk score into human-readable actions.
# Converts numbers ‚Üí decisions
# ‚úî Fully rule-based & explainable
# ‚úî No ML black box
# 
# Interpretation:
# 
# KEEP ‚Üí trusted sample
# 
# REVIEW ‚Üí human-in-the-loop
# 
# REJECT ‚Üí likely mislabeled / problematic

# In[7]:


# ========== SUGGESTION LOGIC ==========
def generate_suggestion(risk):
    if risk >= REJECT_THRESHOLD:
        return "REJECT"
    elif risk >= REVIEW_THRESHOLD:
        return "REVIEW"
    else:
        return "KEEP"

combined_df["suggestion"] = combined_df["combined_risk_score"].apply(generate_suggestion)

print("Suggestion counts:")
print(combined_df["suggestion"].value_counts())


# ## Add Decision Explanation
# ‚úî Makes SLDCE transparent
# ‚úî Enables human trust
# ‚úî Excellent for thesis & demo
# ‚úî Shows why a sample is flagged

# In[8]:


# ========== DECISION EXPLANATION ==========
def explain_decision(row):
    reasons = []

    if row["confidence_flag"]:
        reasons.append("Low confidence in given label")

    if row["anomaly_risk"] >= 0.5:
        reasons.append("Feature anomaly detected")

    if not reasons:
        reasons.append("No strong risk signals")

    return "; ".join(reasons)

combined_df["decision_reason"] = combined_df.apply(explain_decision, axis=1)

combined_df[["combined_risk_score", "suggestion", "decision_reason"]].head()


# ## Save Suggestions Output
# results/
# ‚îî‚îÄ‚îÄ suggestions.csv
# Combined risk score
# 
# Final decision (KEEP / REVIEW / REJECT)
# 
# Human-readable explanation

# In[9]:


# ========== SAVE SUGGESTIONS ==========
results_path = PROJECT_ROOT / "results"
results_path.mkdir(parents=True, exist_ok=True)

output_path = results_path / "suggestions.csv"
combined_df.to_csv(output_path, index=False)

print("Suggestions saved successfully")
print("Saved at:", output_path)


# In[ ]:




